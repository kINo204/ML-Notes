{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca0ef9b0",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Let's say some value $y$ is determined by $y = \\mathbf{\\theta^Tx} + \\varepsilon$. The equation is strictly\n",
    "determined by some natural process, with $\\varepsilon\\sim N(0,r)$ being the total effect of some Gauss noises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a8b2e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.reshape(np.arange(1, 11), shape=(-1, 1))\n",
    "X = np.hstack([x, np.ones_like(x)])  # the design matrix\n",
    "y = 2 * x + 3 + np.random.normal(size=x.shape) / 2  # y = 2x + 3 + normal-noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872b13df",
   "metadata": {},
   "source": [
    "A hypothesis of $y$, given by $h_\\theta(\\mathbf{x}) = \\mathbf{\\theta^Tx}$, is defined on our current belief\n",
    "on the parameter $\\mathbf{\\theta}$.\n",
    "\n",
    "Thus a loss function, MSE, can be built (discussed in detail later):\n",
    "$J(\\mathbf{\\theta}) = \\frac{1}{2}\\Vert y - h_\\theta(\\mathbf{x})\\Vert_2^2$\n",
    "\n",
    "Now with a dataset sampled in the form of $<\\mathbf{X}, \\mathbf{y}>$, the hypothesis and loss can be rewritten as\n",
    "\n",
    "- $\\mathbf{\\~y}=h_\\theta(\\mathbf{X}) = \\mathbf{X}^T\\mathbf{\\theta}$\n",
    "\n",
    "- $J(\\mathbf{\\theta}) = \\sum_i{\\frac{1}{2} \\Vert y_i - h_\\theta(\\mathbf{x}_i)\\Vert_2^2}\n",
    "                    = \\frac{1}{2} \\Vert \\mathbf{y} - \\mathbf{\\~y} \\Vert_2^2\n",
    "                    = \\frac{1}{2} \\Vert (\\mathbf{y} - \\mathbf{X}^T\\theta)^T(\\mathbf{y} - \\mathbf{X}^T\\theta) \\Vert_2^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d08a5992",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = np.zeros((2,1))  # our theta, holding our belief for now\n",
    "hypothesis = lambda: X @ params\n",
    "loss = lambda: (np.sum(np.square(hypothesis() - y))) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c33d45",
   "metadata": {},
   "source": [
    "Note that $J(\\mathbf{\\theta})$ is now a function of $\\mathbf{\\theta}$, and under certain circumstances we may\n",
    "take the gradient of the function on the current point. To minimize the $J(\\mathbf{\\theta})$, we may simply\n",
    "take a step of $\\mathbf{\\theta}$ in the opposite direction of the gradient vector.\n",
    "\n",
    "Through some matrix algebra, we may derive:\n",
    "\n",
    "$\\nabla_\\theta J(\\mathbf{\\theta}) = \\mathbf{X}^T (\\mathbf{X}^T\\mathbf{\\theta} - \\mathbf{y}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8a5635",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = lambda: X.T @ (hypothesis() - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373c421c",
   "metadata": {},
   "source": [
    "then we may take a step as stated above:\n",
    "\n",
    "$\\theta \\gets \\theta - \\alpha \\nabla_\\theta J(\\mathbf{\\theta}) $\n",
    "\n",
    "in which $\\alpha$ is used to scale the gradient vector, and called the \"learning rate\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83abf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params=\n",
      "[[2.01316923]\n",
      " [2.93986322]], loss=1.39\n"
     ]
    }
   ],
   "source": [
    "learn_ratio = 0.003\n",
    "for i in range(10000):\n",
    "    params -= learn_ratio * grad()\n",
    "\n",
    "print(f\"params=\\n{params}, loss={loss():.3}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca0ef9b0",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Let's say some value $y$ is determined by $y = \\bold{\\theta^Tx} + \\varepsilon$. The equation is strictly\n",
    "determined by some natural process, with $\\varepsilon\\sim N(0,r)$ being the total effect of some Gauss noises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8b2e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.reshape(np.arange(1, 11), shape=(-1, 1))\n",
    "X = np.hstack([x, np.ones_like(x)])\n",
    "y = 2 * x + 3 + np.random.normal(size=x.shape) / 2  # y = 2x + 3 + normal-noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872b13df",
   "metadata": {},
   "source": [
    "A hypothesis of $y$, given by $h_\\theta(\\bold{x}) = \\bold{\\theta^Tx}$, is defined on our current belief\n",
    "on the parameter $\\bold{\\theta}$.\n",
    "\n",
    "Thus a loss function, MSE, can be built (discussed in detail later):\n",
    "$J(\\bold{\\theta}) = \\frac{1}{2}\\Vert y - h_\\theta(\\bold{x})\\Vert_2^2$\n",
    "\n",
    "Now with a dataset given in the form of $<\\bold{X}, \\bold{y}>$, the hypothesis and loss can be rewritten as\n",
    "\n",
    "- $\\bold{\\~y}=h_\\theta(\\bold{X}) = \\bold{X}^T\\bold{\\theta}$\n",
    "\n",
    "- $J(\\bold{\\theta}) = \\sum_i{\\frac{1}{2} \\Vert y_i - h_\\theta(\\bold{x}_i)\\Vert_2^2}\n",
    "                    = \\frac{1}{2} \\Vert \\bold{y} - \\bold{\\~y} \\Vert_2^2\n",
    "                    = \\frac{1}{2} \\Vert (\\bold{y} - \\bold{X}^T\\theta)^T(\\bold{y} - \\bold{X}^T\\theta) \\Vert_2^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08a5992",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = np.zeros((2,1))  # our theta, holding our belief for now\n",
    "hypothesis = lambda: X @ params\n",
    "loss = lambda: (np.sum(np.square(hypothesis() - y))) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8a5635",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = lambda: X.T @ (hypothesis() - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83abf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params=\n",
      "[[2.00599422]\n",
      " [3.05789986]], loss=0.262\n"
     ]
    }
   ],
   "source": [
    "learn_ratio = 0.003\n",
    "for i in range(10000):\n",
    "    params -= learn_ratio * grad()\n",
    "\n",
    "print(f\"params=\\n{params}, loss={loss():.3}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
